ReqEmplClassExclude_
a <-available.packages()
warnings()
a <-available.packages()
head(rownames(a), 3)
a <-available.packages()
> head(rownames(a), 15)
a <-available.packages()> head(rownames(a), 15)
a <-available.packages()> head(rownames(a), 15)
a <-available.packages()
> head(rownames(a), 15)
a <-available.packages()
head(rownames(a), 15)
install.packages("rJava")
install.packages("KernSmooth")
library(KernSmooth)
getwd
getwd()
cd "Desktop"
cd "desktop.ini"
setwd("C:\\Users\okalina\Documents\Data Science_Coursera\2.R Programming")
setwd("C:/Users/okalina/Documents/Data Science_Coursera\2.R Programming")
getwd()
util.ReqSummary.subtotals <- function(reqs, SubOrgLevel){
nums <- sapply(reqs, is.numeric)
psts = unique(reqs$CurrentPosting)
psts.tot = rowsum(reqs[, nums], reqs$CurrentPosting)
psts.tot = cbind(psts.tot[,1],psts.tot[,1],psts.tot)
colnames(psts.tot) <- colnames(reqs)
psts.tot$CurrentPosting = NA
psts.tot[,SubOrgLevel] = paste("Total:", psts)
reqs.all = rbind(reqs[which(reqs$CurrentPosting==psts[1]),],
psts.tot[1,])
for (n in 2:length(psts)) {
reqs.all = rbind(reqs.all,
reqs[which(reqs$CurrentPosting==psts[n]),],
psts.tot[n,])
}
reqs.all$Total = rowSums(reqs.all[, c(-1,-2)])
return(reqs.all)
}
util.ReqSummary <- function(opens, offers, SubOrgLevel){
SubOrgLevel = SubOrgLevel
OfferReqs = offers$JobRequisition[which(offers$JobApplicationOfferStatus == "Extended" | is.na(offers$JobApplicationOfferStatus))]
opens$CurrentPosting[which(opens$CurrentPosting == "Internal/External")] = "External"
reqs.pending <- opens %>%
group_by_(SubOrgLevel, 'EmployeeClass', 'CurrentPosting') %>%
summarise(Positions=sum(CountPendingOffers)) %>%
dcast(paste('CurrentPosting +', SubOrgLevel, '~ EmployeeClass'), sum, value.var = "Positions")
reqs.pending = reqs.pending[!is.na(reqs.pending[,SubOrgLevel]),]
reqs.pending[,1] = as.character(reqs.pending[,1])
# Add totals for each posting type
reqs.pending = util.ReqSummary.subtotals(reqs.pending,SubOrgLevel)
colnames(reqs.pending) <- paste("Pending Offers:", colnames(reqs.pending))
#Immigration holds
reqs.fung <- opens %>%
group_by_(SubOrgLevel, 'EmployeeClass', 'CurrentPosting') %>%
summarise(Positions=sum(CountImmigrationHolds)) %>%
dcast(paste('CurrentPosting +', SubOrgLevel, '~ EmployeeClass'), sum, value.var = "Positions")
reqs.fung = reqs.fung[!is.na(reqs.fung[,SubOrgLevel]),]
reqs.fung[,1] = as.character(reqs.fung[,1])
# Add totals for each posting type
reqs.fung = util.ReqSummary.subtotals(reqs.fung,SubOrgLevel)
colnames(reqs.fung) <- paste("Immigration Holds:", colnames(reqs.fung))
#fungible reqs
reqs.all <- opens %>%
group_by_(SubOrgLevel, 'EmployeeClass', 'CurrentPosting') %>%
summarise(Positions=sum(CountOpenings)) %>%
dcast(paste('CurrentPosting +', SubOrgLevel, '~ EmployeeClass'), sum, value.var = "Positions")
reqs.all = reqs.all[!is.na(reqs.all[,SubOrgLevel]),]
reqs.all[,1] = as.character(reqs.all[,1])
# Add totals for each posting type
reqs.all = util.ReqSummary.subtotals(reqs.all,SubOrgLevel)
colnames(reqs.all) <- paste("Available Openings:", colnames(reqs.all))
nums <- sapply(reqs.all, is.numeric)
reqs.summary = cbind(reqs.pending,reqs.fung[,nums], reqs.all[,nums])
colnames(reqs.summary)[1:2] = word(colnames(reqs.summary)[1:2], start = -1L, sep = fixed(":"))
return(reqs.summary)
}
library(datasets)
data("iris")
data(iris)
?iris
print(iris)
mean_virginica <- mean(Sepal.Length, Species=virginica )
mean(iris[iris$Species == "virginica",]$Sepal.Length)
apply(iris[, 1:4], 2, mean)
colMeans(iris)
apply(iris, 1, mean)
rowMeans(iris[, 1:4])
apply(iris[, 1:4], 1, mean)
apply(iris, 2, mean)
mean(iris[iris$Species == "virginica",]$Sepal.Length)
apply(iris[, 1:4], 2, mean)
library(datasets)
data(mtcars)
?mtcars
print(mtcars)
tapply(mtcars$mpg, mtcars$cyl, mean)
new <- tapply(mtcars$hp, mtcars$cyl, mean)
round(abs(new[3]-new[1]))
mean(mtcars[mtcars$cyl == "8",]$hp) - mean(mtcars[mtcars$cyl == "4",]$hp)
debug(ls)
ls
employee <- "leila"
employee <- "leila"
employee
typeof(employee)
EmployeeName_V <- c("leila", "Jhon", "Mike")
EmployeeName_V <- c("leila", "Jhon", "Mike")
EmployeeAge_V <- c(35, 23, 38)
Employee_List <- list (EmployeeName_V, EmployeeAge_V)
Employee_List
Employee_DataFrame
Employee_DataFrame <- data.frame(EmployeeName_V, EmployeeAge_V)
Employee_DataFrame
Employee_DataFrame[1,1]
Employee_DataFrame$EmployeeName_V
install.packages("dplyr")
library("dplyr")
select
select(iris, contains("Wi"))
view(iris)
View(iris)
iris %>% group_by(Species) %>%   ## get iris data after that group it based on Species
summarise(avg = mean(Petal.Width))
arrange(avg)
iris %>% group_by(Species) %>%   ## get iris data after that group it based on Species
summarise(avg = mean(Petal.Width)) %>%  ## after that summarize by mean of Petal width
arrange(avg)
install.packages("RODBC")
library("RODBC")
DBHANDLE<-odbcDriverConnect('driver={SQL Server};server=localhost;database=DW;trusted_connection=true')
res<-sqlQuery(DBHANDLE,'select * from usedcars')
res
library(readr)
usedcars <- read_csv("~/Advanced Analytics with Power BI and R/Dataset/usedcars.csv")
View(usedcars)
summary(usedcars)
summary(usedcars$price)
# Input load. Please do not change #
`dataset` = read.csv('C:/Users/okalina/AppData/Local/Radio/REditorWrapper_5af888a0-cefe-4557-92d3-29233cadf30c/input_df_f9c917a0-f5c3-43e6-8008-228246a71f8e.csv', check.names = FALSE, encoding = "UTF-8", blank.lines.skip = FALSE);
# Original Script. Please update your script content here and once completed copy below section back to the original editing window #
boxplot(dataset$mileage)
install.packages("ggplot2")
library("ggplot2")
View(mpg)
f <- ggplot(mpg, aes(cty, hwy))
f + geom_blank()
f + geom_jitter()
f+geom_bin2d(binwidth=c(6,6))
f+geom_density2d(
f+geom_density2d()
f+geom_bin2d(binwidth=c(2,6))
f+geom_bin2d(binwidth=c(2,6))
f+geom_density2d()
library(ggplot2)
t<-ggplot(mpg, aes(x=cty, y=hwy,size=cyl)) + geom_point(pch=21)+scale_size_continuous(range=c(1,5))
t
library(ggplot2)
t<-ggplot(mpg, aes(x=cty, y=hwy,size=cyl)) + geom_point(pch=21)+scale_size_continuous(range=c(1,10))
t
install.packages("reshape2")
library("reshape2")
t<-t + facet_wrap(year~ drv)
t
t<-ggplot(mpg, aes(x=cty, y=hwy,color=cyl)) + geom_point(size=5)
t<-t + facet_wrap(year~ drv)
t
t<-ggplot(dataset, aes(x=cty, y=hwy,color=cyl)) + geom_point(size=5)
t<-t + facet_wrap(year~ drv)
t
install.packages("PerformanceAnalytics")
library("PerformanceAnalytics")
install.packages("class")
library("class")
install.packages("Matrix")
library("Matrix")
library(arules)
library(methods)
groceries <- read.transactions("C://Users//okalina//Documents//Advanced Analytics with Power BI and R//Dataset//groceries.csv", sep = ",")
Temp<-apriori(groceries, parameter = list(support = 0.006, confidence = 0.25, minlen = 2))
output<-inspect(Temp[1:100])
install.packages("arules")
library("arules")
kings <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
kingTSObj<-ts(kings)
plot.ts(kingTSObj)
http://robjhyndman.com/tsdldata/data/nybirths.dat
births <- scan("http://robjhyndman.com/tsdldata/data/nybirths.dat")
birthTSObj<-ts(births,frequency=12, start=c(1946,1))
plot.ts(birthTSObj)
birthTSDecompose <- decompose(birthTSObj)
plot(birthTSDecompose)
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)
plot.ts(rainTSObj)
rainForecast<-HoltWinters(rainTSObj,beta=FALSE,gamma = FALSE)
rainForecast$fitted
plot(rainForecast)
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)
rainJSObj<-ts(rain)
plot.ts(rainTSObj)
rainForecast<-HoltWinters(rainTSObj,beta=FALSE,gamma = FALSE)
rainForecast$fitted
plot(rainForecast)
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)
rainTJSObj<-ts(rain)
plot.ts(rainTSObj)
rainForecast<-HoltWinters(rainTSObj,beta=FALSE,gamma = FALSE)
rainForecast$fitted
plot(rainForecast)
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)
rainTJSObj<-ts(rain)
rainForecast<-HoltWinters(rainTSObj,beta=FALSE,gamma = FALSE)
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)
rainTSObj<-ts(rain)
plot.ts(rainTSObj)
rainForecast<-HoltWinters(rainTSObj,beta=FALSE,gamma = FALSE)
rainForecast$fitted
plot(rainForecast)
install.packages("forecast")
library("forecast")
rainForecast<-forecast.HoltWinters(rainForecast,h=8)
plot.forecast(rainForecast)
install.packages("forecast")
library("forecast")
rainForecast<-forecast.HoltWinters(rainForecast,h=8)
plot.forecast(rainForecast)
install.packages("forecast")
install.packages("forecast")
library("forecast")
rainForecast<-forecast.HoltWinters(rainForecast,h=8)
plot.forecast(rainForecast)
install.packages("plotly")
library("plotly")
install.packages("htmlwidgets")
install.packages("htmlwidgets")
library("htmlwidgets")
install.packages("rpart.plot")
library("rpart.plot")
install.packages("installr") # install
setInternet2(TRUE) # only for R versions older than 3.3.0
installr::updateR() # updating R.
# If you wish it to go faster, run: installr::updateR(T)
install.packages("installr") # install
setInternet2(TRUE) # only for R versions older than 3.3.0
installr::updateR() # updating R.
# If you wish it to go faster, run: installr::updateR(T)
install.packages("installr")
library(shiny)
library(tm)
library(SnowballC)
library(randomForest)
rfNews()
options(shiny.maxRequestSize=3*1024^2)
options(mc.cores=1)
build_model <- function(new_data_df, sparsity) {
# Create new data corpus
new_corpus <- Corpus(VectorSource(new_data_df$Text))
new_corpus <- tm_map(new_corpus, content_transformer(tolower))
new_corpus <- tm_map(new_corpus, removePunctuation)
new_corpus <- tm_map(new_corpus, removeWords, stopwords("english"))
new_corpus <- tm_map(new_corpus, stripWhitespace)
new_corpus <- tm_map(new_corpus, stemDocument)
message("build_model: corpus DONE")
# create document-term matrix
new_dtm <- DocumentTermMatrix(new_corpus)
new_dtm <- removeSparseTerms(new_dtm, sparsity)
new_dtm_df <- as.data.frame(as.matrix(new_dtm))
colnames(new_dtm_df) <- make.names(colnames(new_dtm_df))
message("build_model: ", "term matrix created for new data with ", ncol(new_dtm_df), " variables")
# intersect corpora and prepare final training data
common_names = intersect(colnames(train_dtm_df),colnames(new_dtm_df))
new_dtm_df <- subset(new_dtm_df, select=names(new_dtm_df) %in% common_names)
message("build_model: ", "new data term matrix reduced to ", ncol(new_dtm_df), " variables")
model_train_data_df <- cbind(train_data_df, subset(train_dtm_df, select=names(train_dtm_df) %in% common_names))
model_train_data_df$Text <- NULL
message("build_model: ", "final training data created with ", ncol(model_train_data_df)-1, " variables")
# train classifier
message("build_model: ", "training classifier...")
model <- randomForest(Sentiment~.,data=model_train_data_df, ntree=50)
message("build_model: ", "classifier training DONE!")
list(model, new_dtm_df)
}
shinyServer(function(input, output) {
output$contents <- renderTable({
results()
})
output$status <- renderText({
if (!is.null(train_dtm_df))
return("Ready!")
return("Starting...")
})
output$distribution <- renderPlot({
if (is.null(results()))
return(NULL)
d <- density(
as.numeric(results()$Prob > input$threshold)
)
plot(
d,
xlim = c(0, 1),
main=paste0("Sentiment Distribution (Prob > ", input$threshold, ")")
)
polygon(d, col="lightgrey", border="lightgrey")
abline(v = input$threshold, col = "blue")
})
results <- reactive({
inFile <- input$file1
if (is.null(inFile))
return(NULL)
# load input data
new_data_df <- read.csv(
inFile$datapath,
sep='\t',
header=FALSE,
quote = "",
stringsAsFactor=F,
col.names=c("Text")
)
message("renderTable: ", "input file loaded")
model_and_data <- build_model(new_data_df, input$sparsity)
message("renderTable: ", "making predictions...")
pred <- predict(model_and_data[[1]], newdata=model_and_data[[2]], type="prob")
message("renderTable: ", "predictions DONE")
new_data_df$Prob <- pred[,2]
new_data_df
})
})
# Load train and test data
train_data_df <- read.csv(
file = 'train_data.tsv',
sep='\t',
quote = "",
header=FALSE,
stringsAsFactor=F,
col.names=c("Sentiment", "Text")
)
train_data_df$Sentiment <- as.factor(train_data_df$Sentiment)
message(paste("There are", nrow(train_data_df), "rows in training dataset"))
# Create training corpus for later re-use
train_corpus <- Corpus(VectorSource(train_data_df$Text))
#message("init: corpus created with length ", length(train_corpus))
train_corpus <- tm_map(train_corpus, content_transformer(tolower))
#message("init: corpus lowercased with length ", length(train_corpus))
train_corpus <- tm_map(train_corpus, removePunctuation)
#message("init: corpus punctuation removed with length ", length(train_corpus))
train_corpus <- tm_map(train_corpus, removeWords, stopwords("english"))
#message("init: corpus stopwords removed with length ", length(train_corpus))
train_corpus <- tm_map(train_corpus, stripWhitespace)
#message("init: corpus space stripped with length ", length(train_corpus))
train_corpus <- tm_map(train_corpus, stemDocument)
#message("init: corpus stemmed with length ", length(train_corpus))
message("init: training corpus DONE")
# create document-term matrix
train_dtm <- DocumentTermMatrix(train_corpus)
train_dtm <- removeSparseTerms(train_dtm, 0.995)
message(paste0("init: training dtm created (", ncol(train_dtm), " terms in training corpus)"))
train_dtm_df <- data.frame(as.matrix(train_dtm))
message(paste0("init: training dtm converted to df (", ncol(train_dtm), " terms in training corpus)"))
colnames(train_dtm_df) <- make.names(colnames(train_dtm_df))
message(paste0("init: training dtm DONE (", ncol(train_dtm_df), " terms in training corpus)"))
function(input, output, session) {
# Combine the selected variables into a new data frame
selectedData <- reactive({
iris[, c(input$xcol, input$ycol)]
})
clusters <- reactive({
kmeans(selectedData(), input$clusters)
})
output$plot1 <- renderPlot({
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
"#FF7F00", "#FFFF33", "#A65628", "#F781BF", "#999999"))
par(mar = c(5.1, 4.1, 0, 1))
plot(selectedData(),
col = clusters()$cluster,
pch = 20, cex = 3)
points(clusters()$centers, pch = 4, cex = 4, lwd = 4)
})
}
pageWithSidebar(
headerPanel('Iris k-means clustering'),
sidebarPanel(
selectInput('xcol', 'X Variable', names(iris)),
selectInput('ycol', 'Y Variable', names(iris),
selected=names(iris)[[2]]),
numericInput('clusters', 'Cluster count', 3,
min = 1, max = 9)
),
mainPanel(
plotOutput('plot1')
)
)
pageWithSidebar(
headerPanel('iris k-means clustering'),
sidebarPanel(
selectInput('xcol', 'X Variable', names(iris)),
selectInput('ycol', 'Y Variable', names(iris),
selected=names(iris)[[2]]),
numericInput('clusters', 'Cluster count', 3,
min = 1, max = 9)
),
mainPanel(
plotOutput('plot1')
)
)
library(shiny)
function(input, output, session) {
# Combine the selected variables into a new data frame
selectedData <- reactive({
iris[, c(input$xcol, input$ycol)]
})
clusters <- reactive({
kmeans(selectedData(), input$clusters)
})
output$plot1 <- renderPlot({
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
"#FF7F00", "#FFFF33", "#A65628", "#F781BF", "#999999"))
par(mar = c(5.1, 4.1, 0, 1))
plot(selectedData(),
col = clusters()$cluster,
pch = 20, cex = 3)
points(clusters()$centers, pch = 4, cex = 4, lwd = 4)
})
}
pageWithSidebar(
headerPanel('iris k-means clustering'),
sidebarPanel(
selectInput('xcol', 'X Variable', names(iris)),
selectInput('ycol', 'Y Variable', names(iris),
selected=names(iris)[[2]]),
numericInput('clusters', 'Cluster count', 3,
min = 1, max = 9)
),
mainPanel(
plotOutput('plot1')
)
)
getwd()
setwd("C:/Users/okalina/GitDocuments/ProgrammingAssignment2")
getwd()
## A pair of functions that cache the inverse of a matrix.
## This function creates a special "matrix" object that can cache its inverse.
makeCacheMatrix <- function(x = matrix()) {
inv <- NULL
set <- function(y){
x <<- y
inv <<- NULL
}
get <- function() x
setInverse <- function(solveMatrix) inv <<- solveMatrix
getInverse <- function() inv
list(set = set, get = get, setInverse = setInverse, getInverse = getInverse)
}
## This function computes the inverse of the special "matrix" returned by makeCacheMatrix above
## However, it first checks to see if the inverse has already been calculated
## If yes, it gets it from the cache and skips the computation
## If not, the function calculates the inverse of the matrix and sets the value in the cache
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
inv <- x$getInverse()
if(!is.null(inv)){
message("getting cached data")
return(inv)
}
data <- x$get()
inv <- solve(data)
x$setInverse(inv)
inv
}
test = function(mat){
## @mat: an invertible matrix
temp = makeCacheMatrix(mat)
start.time = Sys.time()
cacheSolve(temp)
dur = Sys.time() - start.time
print(dur)
start.time = Sys.time()
cacheSolve(temp)
dur = Sys.time() - start.time
print(dur)
}
set.seed(1110201)
r = rnorm(1000000)
mat1 = matrix(r, nrow=1000, ncol=1000)
test(mat1)
